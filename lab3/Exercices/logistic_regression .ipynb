{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1562605986.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 18\u001b[0;36m\u001b[0m\n\u001b[0;31m    x_train = ### YOUR CODE GOES HERE\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# In this exercise, we are going to classify the breast cancer wisconsin dataset.\n",
    "# This dataset contains in total 569 examples, among them 212 examples are labelled as malignant (M or 0) and 357 \n",
    "# examples are marked as benign (B or 1). .Each example is a vector of 30 dimensions.\n",
    "# We will train a binary logistic regression model using this dataset.\n",
    "\n",
    "# Load, normalize, split and visualize your dataset. This step has been done for you.\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the dataset\n",
    "data = load_breast_cancer()\n",
    "x = data.data\n",
    "y = data.target\n",
    "\n",
    "# split into training and test sets, namely 80 percent of examples goes for the training, \n",
    "# 20 percent goes for the test set\n",
    "x_train = ### YOUR CODE GOES HERE \n",
    "y_train = ### YOUR CODE GOES HERE \n",
    "x_test = ### YOUR CODE GOES HERE \n",
    "y_test = ### YOUR CODE GOES HERE \n",
    "\n",
    "# scale features by removing mean and dividing by the standard deviation\n",
    "# use only the statistics from the training set\n",
    "x_train_scaled = ### YOUR CODE GOES HERE \n",
    "x_test_scaled = ### YOUR CODE GOES HERE \n",
    "\n",
    "print(\"Number of training examples: \",x_train_scaled.shape[0])\n",
    "print(\"Number of testing examples: \",x_test_scaled.shape[0])\n",
    "\n",
    "# visualize the dataset using histogram\n",
    "labels = ['Benign','Malignant']\n",
    "population = [np.sum(y),np.sum(y==0)]\n",
    "y_pos = np.arange(len(labels))\n",
    "barlist = plt.bar(y_pos, population, align='center',width=0.3)\n",
    "plt.xticks(y_pos, labels)\n",
    "plt.ylabel('Number of examples')\n",
    "plt.title('Breast wisconsin dataset.')\n",
    "barlist[1].set_color('r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add intercept terms and initialize parameters\n",
    "x_train_scaled = ### YOUR CODE GOES HERE \n",
    "x_test_scaled = ### YOUR CODE GOES HERE \n",
    "\n",
    "print(x_train_scaled.shape)\n",
    "print(x_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Implement the sigmoid, gradient and cost functions\n",
    "\n",
    "# this function returns the probability of y=1\n",
    "# x: data matrix\n",
    "# theta: model's parameters\n",
    "def sigmoid(x,theta):\n",
    "    ### YOUR CODE GOES HERE ###\n",
    "\n",
    "# logarithmic loss\n",
    "# x: data matrix (2D)\n",
    "# y: label (1D)\n",
    "# theta: model's parameters (1D)\n",
    "def compute_cost(x,y,theta):\n",
    "    ### YOUR CODE GOES HERE ###\n",
    "\n",
    "def compute_gradient(x,y,theta):\n",
    "    ### YOUR CODE GOES HERE ###\n",
    "\n",
    "def approximate_gradient(x,y,theta,epsilon):\n",
    "    n_features = x.shape[1]\n",
    "    app_grad = np.zeros(n_features)\n",
    "    for i in range(n_features):\n",
    "        epsilon_one_hot = np.zeros(n_features)\n",
    "        epsilon_one_hot[i] = epsilon\n",
    "        theta_before = theta - epsilon_one_hot\n",
    "        theta_after = theta + epsilon_one_hot\n",
    "        app_grad[i] = (compute_cost(x,y,theta_after) - compute_cost(x,y,theta_before))/(2*epsilon)\n",
    "    return app_grad\n",
    "\n",
    "theta = 0.5 * np.random.randn(x_train_scaled.shape[1])\n",
    "grad = compute_gradient(x_train_scaled,y_train,theta)\n",
    "epsilon = 1e-4\n",
    "app_grad = approximate_gradient(x_train_scaled,y_train,theta,epsilon)\n",
    "print('Sum of gradient squared error: ',np.sum((grad - app_grad)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Update model parameters using mini-batch gradient descent algorithm\n",
    "\n",
    "# try different values for the learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# number of training iterations\n",
    "num_samples = x_train_scaled.shape[0]\n",
    "batch_size = int(num_samples/10)\n",
    "N_iterations = int(num_samples / batch_size) * 50  # 50 epochs\n",
    "\n",
    "# prepare to plot\n",
    "plt.subplot(111)\n",
    "\n",
    "# keep the cost value for each training step\n",
    "J = np.zeros(N_iterations)\n",
    "\n",
    "# initialize new parameters using random distribution\n",
    "theta_msgd = ### YOUR CODE GOES HERE ###\n",
    "\n",
    "start = 0\n",
    "end = 0\n",
    "for step in range(N_iterations):\n",
    "    if step % (num_samples / batch_size) == 0:\n",
    "        # shuffle the training data \n",
    "        ### YOUR CODE GOES HERE ###\n",
    "        \n",
    "        # indices = np.random.permutation(num_samples)\n",
    "        # x_train_scaled = x_train_scaled[indices,:]\n",
    "        # y_train = y_train[indices]\n",
    "\n",
    "    # create a mini-batch of data to train on\n",
    "    x_batch = ### YOUR CODE GOES HERE ###\n",
    "    y_batch = ### YOUR CODE GOES HERE ###\n",
    "   \n",
    "    # calculate the cost on x_step and y_step\n",
    "    J[step] = ### YOUR CODE GOES HERE ###\n",
    "    \n",
    "    # update theta_msgd using a x_step and y_step\n",
    "    theta_msgd = ### YOUR CODE GOES HERE ###\n",
    "\n",
    "# calculate the loss on the whole training set \n",
    "J_train = compute_cost(x_train_scaled, y_train, theta_msgd)\n",
    "print('training cost: %f' %J_train)\n",
    "# plot cost function\n",
    "plt.plot(J)\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Predict and draw the confusion matrix\n",
    "# calculate the accuracy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# this function draw the confusion matrix, you don't need to touch it.\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# this function computes the accuracy, which is the percentage of the correctly classified examples\n",
    "# y_ground_truth: original labels (1D vector)\n",
    "# y_pre: predicted labels (1D vector)\n",
    "# return: accuracy in percent\n",
    "def compute_accuracy(y_ground_truth,y_pred):\n",
    "    ### YOUR CODE GOES  HERE ###\n",
    "    \n",
    "# In logistic regression, we choose a threshold.\n",
    "# If the output of our hypothesis is greater than this threshold, the example is classified as 1, \n",
    "# otherwise it is given 0.\n",
    "\n",
    "threshold = ### YOUR CODE GOES HERE ###\n",
    "y_pred = ### YOUR CODE GOES HERE ###\n",
    "print(\"Accuracy on test set: {:.2f}\".format(compute_accuracy(y_test,y_pred)))\n",
    "\n",
    "# calculate the cost for the test set\n",
    "test_cost = compute_cost(x_test_scaled,y_test,theta_msgd)\n",
    "print('Test cost: ',test_cost)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=labels,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
