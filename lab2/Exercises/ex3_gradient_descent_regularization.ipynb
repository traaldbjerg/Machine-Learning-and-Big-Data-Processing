{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataset, split into training and test sets, scale features and add intercept terms\n",
    "# This step has been don for you because we \n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "\n",
    "# load boston housing price dataset\n",
    "boston = load_boston()\n",
    "x = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# split into training and test sets, namely 80 percent of examples goes for the training, 20 percent goes for the test set\n",
    "N_train = int(0.8 * x.shape[0])\n",
    "x_train = x[:N_train,:]\n",
    "y_train = y[:N_train]\n",
    "x_test = x[N_train:,:]\n",
    "y_test = y[N_train:]\n",
    "\n",
    "# scale features by removing mean and dividing by the standard deviation\n",
    "x_bar = np.mean(x_train,axis=0)\n",
    "x_std = np.std(x_train,axis=0)\n",
    "x_train_scaled = (x_train - x_bar)/x_std\n",
    "x_test_scaled = (x_test - x_bar)/x_std\n",
    "\n",
    "print('Number of training samples: ',x_train_scaled.shape[0])\n",
    "print('Number of testing samples: ',x_test_scaled.shape[0])\n",
    "\n",
    "# add intercept term\n",
    "intercept_train = np.ones((N_train,1))\n",
    "x_train_scaled = np.hstack((intercept_train,x_train_scaled))\n",
    "intercept_test = np.ones((x.shape[0] - N_train,1))\n",
    "x_test_scaled = np.hstack((intercept_test,x_test_scaled))\n",
    "\n",
    "print('Training set shape: ',x_train_scaled.shape)\n",
    "print('Testing set shape: ',x_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Implement the gradient and the cost function with regularization term\n",
    "# In this step, you have to modify the gradient and the cost functions by adding regularization term\n",
    "# After that, you verify your implementation using function approximate_gradient implemented in the previous exercise\n",
    "def compute_regularized_gradient(x,y,theta,lambda_):\n",
    "    ### YOUR CODE GOES HERE ###\n",
    "\n",
    "def compute_regularized_cost(x,y,theta,lambda_):\n",
    "    ### YOUR CODE GOES HERE ###\n",
    "\n",
    "def approximate_gradient(x,y,theta,epsilon,lambda_):\n",
    "    n_features = x.shape[1]\n",
    "    app_grad = np.zeros(n_features)\n",
    "    for i in range(n_features):\n",
    "        epsilon_one_hot = np.zeros(n_features)\n",
    "        epsilon_one_hot[i] = epsilon\n",
    "        theta_before = theta - epsilon_one_hot\n",
    "        theta_after = theta + epsilon_one_hot\n",
    "        app_grad[i] = (compute_regularized_cost(x,y,theta_after,lambda_) - compute_regularized_cost(x,y,theta_before,lambda_))/(2*epsilon)\n",
    "    return app_grad\n",
    "\n",
    "# verify your implementation\n",
    "theta = 0.5 * np.random.randn(x_train_scaled.shape[1])\n",
    "regularization_term = 1\n",
    "grad = compute_regularized_gradient(x_train_scaled,y_train,theta,regularization_term)\n",
    "epsilon = 1e-4\n",
    "app_grad = approximate_gradient(x_train_scaled,y_train,theta,epsilon,regularization_term)\n",
    "print('Sum of gradient squared error: ',np.sum((grad - app_grad)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Try gradient descent algorithm with different regularization values\n",
    "# In this step, you train your model with different lambda values, then you plot the cost for training set and testing\n",
    "# set on the same figure. You can try to figure out the effect of adding the regularization term.\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "\n",
    "# learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# regularization term\n",
    "lambdas = [0,0.1,1,10]\n",
    "\n",
    "# this matrix keeps the learned parameters\n",
    "theta_matrix = np.zeros((len(lambdas),x_train_scaled.shape[1]))\n",
    "\n",
    "# number of training iterations\n",
    "N_iterations = 40\n",
    "\n",
    "# prepare to plot\n",
    "fig = plt.subplot(111)\n",
    "ax = plt.gca()\n",
    "\n",
    "# calculate cost value and update theta\n",
    "lines = []\n",
    "for indx,lambda_ in enumerate(lambdas):\n",
    "    # keep the cost value for each training step\n",
    "    J_train = np.zeros(N_iterations)\n",
    "    J_test = np.zeros(N_iterations)\n",
    "    \n",
    "    # initialize new parameters using random distribution\n",
    "    theta = 0.5 * np.random.randn(x_train_scaled.shape[1])\n",
    "    for step in range(N_iterations):\n",
    "        # update theta\n",
    "        theta = ### YOUR CODE GOES HERE ###\n",
    "        \n",
    "        # calculate the cost on traing set\n",
    "        J_train[step] = ### YOUR CODE GOES HERE ###\n",
    "        \n",
    "        # calculate cost on the testing set\n",
    "        J_test[step] = ### YOUR CODE GOES HERE ###\n",
    "        \n",
    "    # save the value of theta\n",
    "    theta_matrix[indx,:] = theta\n",
    "\n",
    "    # use this color for both training and testing cost\n",
    "    color = next(ax._get_lines.prop_cycler)['color']\n",
    "    # plot cost function\n",
    "    line1 = plt.plot(J_train,color=color)\n",
    "    line2 = plt.plot(J_test,'--',color=color)\n",
    "    lines.append((line1,line2))\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Cost')\n",
    "labels = []\n",
    "for lbd in lambdas:\n",
    "    labels.append('train_lambda_' + str(lbd))\n",
    "    labels.append('test_lambda_' + str(lbd))\n",
    "plt.legend(labels, loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Choose lambda\n",
    "# In this step, you select the best regularization parameter by calculating cost on the test set.\n",
    "# You can find the list of lambda values from the previous cell\n",
    "lambda_index = 0\n",
    "theta = theta_matrix[lambda_index,:]\n",
    "predict_price = np.dot(x_test_scaled,theta)\n",
    "\n",
    "# calculate the cost for the test set with lambda=0\n",
    "test_cost = compute_regularized_cost(x_test_scaled,y_test,theta,0)\n",
    "print('test cost: ',test_cost)\n",
    "\n",
    "# plot the ground truth and the predicted\n",
    "x_axis = np.linspace(1,len(y_test),len(y_test))\n",
    "plt.plot(x_axis,y_test,'b',x_axis,predict_price,'r')\n",
    "plt.legend(('Ground truth','Predicted'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
