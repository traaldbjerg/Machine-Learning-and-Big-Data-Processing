{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Load dataset, split into training and test sets, and scale features\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# load boston housing price dataset\n",
    "boston = load_boston()\n",
    "x = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# split into training and test sets, namely 80 percent of examples goes for the training, 20 percent goes for the test set\n",
    "N_train = int(0.8 * x.shape[0])\n",
    "x_train = x[:N_train,:]\n",
    "y_train = y[:N_train]\n",
    "x_test = x[N_train:,:]\n",
    "y_test = y[N_train:]\n",
    "\n",
    "# scale features by removing mean and dividing by the standard deviation\n",
    "x_train_scaled = # YOUR CODE GOES HERE\n",
    "x_test_scaled = # YOUR CODE GOES HERE\n",
    "\n",
    "print(x_train_scaled.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test_scaled.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Add intercept terms and initialize parameters\n",
    "# Note: If you run this step again, please run from step 1 because notebook keeps the value from the previous run\n",
    "x_train_scaled = # YOUR CODE GOES HERE\n",
    "\n",
    "x_test_scaled = # YOUR CODE GOES HERE\n",
    "\n",
    "print(x_train_scaled.shape)\n",
    "print(x_test_scaled.shape)\n",
    "\n",
    "# init parameters using random values\n",
    "theta = # YOUR CODE GOES HERE\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Implement the gradient and the cost function\n",
    "# In this step, you have to calculate the gradient. You can use the provided formula but the best way is to vectorize\n",
    "# that formula for efficiency\n",
    "def compute_gradient(x,y,theta):\n",
    "    # YOUR CODE GOES HERE\n",
    "\n",
    "def compute_cost(x,y,theta):\n",
    "    # YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: stochastic gradient descent\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# try different values for the learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# number of training iterations\n",
    "num_samples = x_train_scaled.shape[0]\n",
    "N_iterations = num_samples * 20 # loop over the training dataset 20 times\n",
    "\n",
    "# prepare to plot\n",
    "plt.subplot(111)\n",
    "\n",
    "# calculate cost value and update theta\n",
    "J = np.zeros(N_iterations)\n",
    "\n",
    "# initialize new parameters using random distribution\n",
    "theta_sgd = 0.5 * np.random.randn(x_train_scaled.shape[1])\n",
    "\n",
    "for step in range(N_iterations):\n",
    "    if step % num_samples == 0:\n",
    "        # shuffle the training data (must be done the same way for data and targets)\n",
    "        # YOUR CODE GOES HERE\n",
    "\n",
    "    # select the next sample to train\n",
    "    x_step =  # YOUR CODE GOES HERE\n",
    "    y_step =  # YOUR CODE GOES HERE\n",
    "    x_step = x_step.reshape([1,-1])\n",
    "\n",
    "    # calculate the cost on x_step and y_step\n",
    "    J[step] =  # YOUR CODE GOES HERE\n",
    "\n",
    "    # update theta using a x_step and y_step\n",
    "    theta_sgd =  # YOUR CODE GOES HERE\n",
    "\n",
    "# calculate the loss on the whole training set \n",
    "J_train =  # YOUR CODE GOES HERE\n",
    "print('training cost: %f' %J_train)\n",
    "# plot cost function\n",
    "plt.plot(J)\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 5\n",
    "# Predict the price of house\n",
    "predict_price =  # YOUR CODE GOES HERE\n",
    "\n",
    "# calculate the cost for the test set\n",
    "test_cost =  # YOUR CODE GOES HERE\n",
    "print('test cost: ',test_cost)\n",
    "\n",
    "# plot the ground truth and the prediction\n",
    "x_axis = np.linspace(1,len(y_test),len(y_test))\n",
    "plt.plot(x_axis,y_test,'b',x_axis,predict_price,'r')\n",
    "plt.legend(('Ground truth','Predicted'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 6: mini-batch gradient descent\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# try different values for the learning rate\n",
    "learning_rate = 0.03\n",
    "\n",
    "# number of training iterations\n",
    "num_samples = x_train_scaled.shape[0]\n",
    "batch_size = int(num_samples/10)\n",
    "N_iterations = int(num_samples / batch_size) * 20  # loop over the training dataset 20 times\n",
    "\n",
    "# prepare to plot\n",
    "plt.subplot(111)\n",
    "\n",
    "# keep the cost value for each training step\n",
    "J = np.zeros(N_iterations)\n",
    "\n",
    "# initialize new parameters using random distribution\n",
    "theta_msgd = 0.5 * np.random.randn(x_train_scaled.shape[1])\n",
    "\n",
    "start = 0\n",
    "end = 0\n",
    "for step in range(N_iterations):\n",
    "    if step % num_samples == 0:\n",
    "        # shuffle the training data \n",
    "        # YOUR CODE GOES HERE\n",
    "\n",
    "    # create a mini-batch of data to train on\n",
    "    end = start + batch_size\n",
    "    if end >= num_samples:\n",
    "        end = num_samples\n",
    "    # slice x_train_scaled and y_train from start to end \n",
    "    # to create training batch\n",
    "    x_batch =  # YOUR CODE GOES HERE\n",
    "    y_batch =  # YOUR CODE GOES HERE\n",
    "    start = 0 if end >= num_samples else end\n",
    "    \n",
    "    # calculate the cost on x_step and y_step\n",
    "    J[step] =  # YOUR CODE GOES HERE\n",
    "\n",
    "    # update theta_msgd using a x_step and y_step\n",
    "    theta_msgd =  # YOUR CODE GOES HERE\n",
    "\n",
    "# calculate the loss on the whole training set \n",
    "J_train =  # YOUR CODE GOES HERE\n",
    "print('training cost: %f' %J_train)\n",
    "# plot cost function\n",
    "plt.plot(J)\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ste 7\n",
    "# Predict the price of house\n",
    "predict_price =  # YOUR CODE GOES HERE\n",
    "\n",
    "# calculate the cost for the test set\n",
    "test_cost =  # YOUR CODE GOES HERE\n",
    "print('test cost: ',test_cost)\n",
    "\n",
    "# plot the ground truth and the prediction\n",
    "x_axis = np.linspace(1,len(y_test),len(y_test))\n",
    "plt.plot(x_axis,y_test,'b',x_axis,predict_price,'r')\n",
    "plt.legend(('Ground truth','Predicted'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
